{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis de Negocio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Objetivo del Modelo:**\n",
        "El modelo está diseñado para generar texto similar al de \"Don Quijote de la Mancha\". Sus aplicaciones potenciales incluyen:\n",
        "- Generación de contenido: Crear textos literarios o inspirados en obras clásicas.\n",
        "- Asistencia a escritores: Proporcionar ideas o continuaciones de frases.\n",
        "- Educación: Enseñar sobre generación de lenguaje natural y procesamiento de texto.\n",
        "\n",
        "**Ventajas:**\n",
        "- Flexibilidad: Puede adaptarse a otros textos con cambios mínimos.\n",
        "- Automatización: Genera contenido rápidamente sin intervención humana.\n",
        "- Personalización: Permite ajustar parámetros como la temperatura para controlar la creatividad del texto generado.\n",
        "\n",
        "**Limitaciones:**\n",
        "- Calidad del texto: Aunque el texto generado es coherente, puede carecer de sentido profundo o contexto preciso.\n",
        "- Dependencia de datos: La calidad del modelo depende en gran medida del texto de entrenamiento.\n",
        "- Recursos computacionales: El entrenamiento de modelos LSTM es costoso en términos de tiempo y hardware.\n",
        "\n",
        "**Oportunidades:**\n",
        "- Integración con herramientas de escritura: Podría incorporarse en editores de texto como sugeridor automático.\n",
        "- Multilingüismo: Entrenar el modelo con textos en diferentes idiomas para ampliar su alcance.\n",
        "- Mejoras con modelos avanzados: Migrar a arquitecturas como Transformers para mejorar la calidad del texto generado.\n",
        "\n",
        "**Riesgos:**\n",
        "- Sobreajuste: El modelo podría memorizar fragmentos del texto de entrenamiento en lugar de generalizar.\n",
        "- Uso ético: La generación automática de contenido debe manejarse con cuidado para evitar plagio o desinformación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL69BSrmO_ow"
      },
      "source": [
        "# Cargar y preprocesar el texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n_LAFzQO7Dk",
        "outputId": "7e4f3d43-264d-477d-c1f4-00dfe115ddc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/2000/2000-0.txt\n",
            "\u001b[1m2226045/2226045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "74 caracteres únicos\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "path = tf.keras.utils.get_file('quijote.txt', 'https://www.gutenberg.org/files/2000/2000-0.txt')\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8').lower()\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} caracteres únicos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQdebHUFPFNu"
      },
      "source": [
        "# Mapear caracteres a enteros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se crean dos diccionarios: **char2idx** para convertir caracteres a índices numéricos y **idx2char** para realizar el proceso inverso. El texto se transforma en una secuencia numérica **(text_as_int)**, lo que permite al modelo procesarlo como una serie de números en lugar de caracteres. Esta representación numérica es esencial para las operaciones matemáticas en el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiNw9QwdPFNv"
      },
      "outputs": [],
      "source": [
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRhoB5dVPF7S"
      },
      "source": [
        "# Crear secuencias de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El texto se divide en secuencias de longitud fija **(seq_length = 100)**, donde cada secuencia se usa para predecir el siguiente carácter. La función **split_input_target** separa cada secuencia en una entrada (todos los caracteres excepto el último) y un objetivo (todos los caracteres excepto el primero). Esto permite entrenar al modelo para predecir el siguiente carácter en una secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv1JI0F6PF7T"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0mSpvNVPGT2"
      },
      "source": [
        "# Preparar batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las secuencias se agrupan en lotes **(BATCH_SIZE = 64)** y se barajan para mejorar el entrenamiento. El uso de batches permite procesar múltiples secuencias simultáneamente, optimizando el uso de recursos computacionales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX--usQzPGT2"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmNoNbHlPHqI"
      },
      "source": [
        "# Crear el modelo LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbPFNWgMPHqJ"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(batch_shape=(batch_size, None)),\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dropout(0.25),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El modelo consta de:\n",
        "- Una capa de Embedding para convertir índices numéricos en vectores densos.\n",
        "- Una capa LSTM con 1024 unidades, que captura dependencias a largo plazo en las secuencias.\n",
        "- Una capa de Dropout para regularización, reduciendo el sobreajuste.\n",
        "- Una capa Dense final para predecir el siguiente carácter.\n",
        "\n",
        "El diseño del modelo es adecuado para la generación de texto, ya que las LSTMs son efectivas para manejar secuencias y patrones temporales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm6GstKbPIDC"
      },
      "source": [
        "# Compilar y entrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El modelo se compila con el optimizador Adam y la función de pérdida **sparse_categorical_crossentropy**. Se divide el dataset en entrenamiento (80%) y validación (20%). Se utilizan callbacks como ModelCheckpoint para guardar pesos y **EarlyStopping** para detener el entrenamiento si no hay mejora en la precisión de validación. El entrenamiento muestra una mejora constante en la precisión y una reducción en la pérdida, tanto en entrenamiento como en validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uODE4dP4PIDC",
        "outputId": "5d66efdd-4a23-4a4a-9c24-351945c07314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2213s\u001b[0m 8s/step - accuracy: 0.2891 - loss: 2.5213 - val_accuracy: 0.4746 - val_loss: 1.7088\n",
            "Epoch 2/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2245s\u001b[0m 8s/step - accuracy: 0.4942 - loss: 1.6346 - val_accuracy: 0.5521 - val_loss: 1.4395\n",
            "Epoch 3/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2258s\u001b[0m 8s/step - accuracy: 0.5511 - loss: 1.4364 - val_accuracy: 0.5827 - val_loss: 1.3315\n",
            "Epoch 4/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2236s\u001b[0m 8s/step - accuracy: 0.5757 - loss: 1.3480 - val_accuracy: 0.5981 - val_loss: 1.2725\n",
            "Epoch 5/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2228s\u001b[0m 8s/step - accuracy: 0.5909 - loss: 1.2945 - val_accuracy: 0.6113 - val_loss: 1.2262\n",
            "Epoch 6/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2229s\u001b[0m 8s/step - accuracy: 0.6009 - loss: 1.2584 - val_accuracy: 0.6214 - val_loss: 1.1918\n",
            "Epoch 7/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2238s\u001b[0m 8s/step - accuracy: 0.6099 - loss: 1.2284 - val_accuracy: 0.6290 - val_loss: 1.1643\n",
            "Epoch 8/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2248s\u001b[0m 8s/step - accuracy: 0.6183 - loss: 1.1996 - val_accuracy: 0.6349 - val_loss: 1.1451\n",
            "Epoch 9/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2148s\u001b[0m 8s/step - accuracy: 0.6240 - loss: 1.1781 - val_accuracy: 0.6414 - val_loss: 1.1262\n",
            "Epoch 10/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2255s\u001b[0m 8s/step - accuracy: 0.6310 - loss: 1.1553 - val_accuracy: 0.6501 - val_loss: 1.0957\n",
            "Epoch 11/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2206s\u001b[0m 8s/step - accuracy: 0.6376 - loss: 1.1372 - val_accuracy: 0.6565 - val_loss: 1.0756\n",
            "Epoch 12/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2218s\u001b[0m 8s/step - accuracy: 0.6427 - loss: 1.1172 - val_accuracy: 0.6611 - val_loss: 1.0623\n",
            "Epoch 13/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2236s\u001b[0m 8s/step - accuracy: 0.6483 - loss: 1.0993 - val_accuracy: 0.6681 - val_loss: 1.0402\n",
            "Epoch 14/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2220s\u001b[0m 8s/step - accuracy: 0.6537 - loss: 1.0828 - val_accuracy: 0.6738 - val_loss: 1.0219\n",
            "Epoch 15/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2196s\u001b[0m 8s/step - accuracy: 0.6606 - loss: 1.0632 - val_accuracy: 0.6812 - val_loss: 0.9982\n",
            "Epoch 16/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2266s\u001b[0m 8s/step - accuracy: 0.6657 - loss: 1.0455 - val_accuracy: 0.6863 - val_loss: 0.9856\n",
            "Epoch 17/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2219s\u001b[0m 8s/step - accuracy: 0.6707 - loss: 1.0282 - val_accuracy: 0.6901 - val_loss: 0.9705\n",
            "Epoch 18/20\n",
            "\u001b[1m268/268\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2232s\u001b[0m 8s/step - accuracy: 0.6767 - loss: 1.0127 - val_accuracy: 0.6977 - val_loss: 0.9517\n",
            "Epoch 19/20\n",
            "\u001b[1m167/268\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m13:00\u001b[0m 8s/step - accuracy: 0.6824 - loss: 0.9933"
          ]
        }
      ],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=loss,\n",
        "    metrics=[\n",
        "        tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "total_batches = dataset.cardinality().numpy()\n",
        "train_size = int(0.8 * total_batches)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "EPOCHS = 20\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_callback, earlystop_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lax_tDiTPIbo"
      },
      "source": [
        "# Funciones para generar texto con el modelo entrenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se implementarán funciones para generar texto:\n",
        "- generate_word: Genera una palabra a partir de un carácter inicial.\n",
        "- generate_phrase: Genera una frase a partir de una palabra inicial.\n",
        "\n",
        "El modelo muestra capacidad para generar texto coherente, aunque con algunos errores gramaticales y de contexto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7BNd0whPIbp",
        "outputId": "88dd1130-0b42-4b83-ca30-74b562870506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En un lugar de la mancha, y\r\n",
            "que el que de bronce, hecha un pas de los caballones gánedas y\r\n",
            "señoras. tú mis lados de la tierra, y mis costillas puntualmente —ue le\r\n",
            "hallamos— yo soy el borroso historiador está por la camisa, atóndiente que tiene las niñas o\r\n",
            "amadís. ¡bon, tr sinas oblinados niñe-''''.fito que la de cosas, y que mi servente le hubiese\r\n",
            "ala y muerto, sin posibilitarse,\r\n",
            "al persando y se entraron, don quijote y no habéis de estar\r\n",
            "todo, y en la de tantos\r\n",
            "fuertes hingas.\r\n",
            "\r\n",
            "— deteneos, soy de mi tridad e\n"
          ]
        }
      ],
      "source": [
        "def generate_word(model, start_char, temperature=1.0, max_chars=30):\n",
        "    input_eval = [char2idx[c] for c in start_char.lower() if c in char2idx]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    model.layers[1].reset_states()\n",
        "\n",
        "    word = start_char\n",
        "    for _ in range(max_chars):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0) / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        next_char = idx2char[predicted_id]\n",
        "        word += next_char\n",
        "\n",
        "        if next_char == ' ':\n",
        "            break\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return word.strip()\n",
        "\n",
        "\n",
        "def generate_phrase(model, start_word, temperature=1.0, max_words=10):\n",
        "    current_input = start_word\n",
        "    phrase = start_word.strip()\n",
        "\n",
        "    for _ in range(max_words - 1):\n",
        "        next_word = generate_word(model, current_input[-1], temperature=temperature)\n",
        "        phrase += ' ' + next_word\n",
        "        current_input = next_word\n",
        "\n",
        "    return phrase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluación del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Resultados del Entrenamiento:**\n",
        "\n",
        "**Precisión (Accuracy):**\n",
        "- Entrenamiento: Aumentó de ~29% en la primera época a ~68% en la última.\n",
        "- Validación: Mejoró de ~47% a ~69%, mostrando que el modelo generaliza bien.\n",
        "\n",
        "**Pérdida (Loss):**\n",
        "- Entrenamiento: Reducción de 2.52 a 1.01.\n",
        "- Validación: Reducción de 1.71 a 0.97.\n",
        "\n",
        "**Interpretación:**\n",
        "- El modelo aprendió efectivamente a predecir el siguiente carácter en una secuencia, como lo demuestra el aumento en la precisión y la reducción en la pérdida.\n",
        "- La pequeña brecha entre las métricas de entrenamiento y validación indica que no hubo sobreajuste significativo.\n",
        "\n",
        "**Generación de Texto:**\n",
        "\n",
        "- Fortalezas: El texto mantiene un estilo similar al original, con estructura gramatical aceptable.\n",
        "- Debilidades: Algunas palabras o frases carecen de sentido (ej. \"caballones gánedas\").\n",
        "\n",
        "**Cómo mejorariamos el modelo:**\n",
        "- Ajustar Hiperparámetros: Probar con más unidades LSTM o mayor dimensión de embedding para mejorar la calidad del texto.\n",
        "- Aumentar Datos: Incluir más textos literarios para enriquecer el vocabulario y contexto.\n",
        "- Regularización: Aumentar el dropout o usar técnicas como weight decay para evitar sobreajuste.\n",
        "\n",
        "**Conclusión:**\n",
        "El modelo logra su objetivo de generar texto inspirado en \"Don Quijote\", con un rendimiento sólido en términos de precisión y pérdida. Sin embargo, hay margen para mejorar la coherencia y relevancia del texto generado. Con ajustes y más datos, podría convertirse en una herramienta poderosa para aplicaciones creativas y educativas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
