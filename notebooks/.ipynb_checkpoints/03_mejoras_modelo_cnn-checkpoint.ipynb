{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be53922-535f-497a-8b0a-98f866b8d6ce",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Mejora del modelo CNN con Data Augmentation y Regularizaci√≥n Avanzada\n",
    "Notebook 03.5 ‚Äì Mejora adicional del modelo CNN\n",
    "En este notebook aplicamos t√©cnicas adicionales para mejorar el rendimiento del modelo CNN:\n",
    "\n",
    "- Aumento de datos avanzado (Cutout)\n",
    "- Arquitectura mejorada con m√°s filtros\n",
    "- Esquema de optimizaci√≥n c√≠clico\n",
    "- Label Smoothing\n",
    "- Normalizaci√≥n mejorada\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac867a5-f789-438b-8988-eed825a9e9ad",
   "metadata": {},
   "source": [
    "## üîÅ 1. Carga y preprocesamiento de datos\n",
    "Cargamos el dataset CIFAR-100 y aplicamos normalizaci√≥n avanzada usando la media y desviaci√≥n est√°ndar del conjunto de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92980a74-b795-46fe-9204-f2a992019414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "# Normalizar (m√©todo simple pero efectivo)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_cat = to_categorical(y_train, 100)\n",
    "y_test_cat = to_categorical(y_test, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f791756-18a1-41a6-a1bb-fcc16a631e22",
   "metadata": {},
   "source": [
    "## üîç 2. Normalizaci√≥n mejorada\n",
    "Mejoramos la normalizaci√≥n calculando la media y desviaci√≥n est√°ndar del conjunto de entrenamiento para cada canal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d7e0ccb-5e6a-4d7c-b6ae-d9e0d7e1955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media por canal: [0.5070754  0.48655024 0.44091907]\n",
      "Desviaci√≥n est√°ndar por canal: [0.26733398 0.25643876 0.2761503 ]\n"
     ]
    }
   ],
   "source": [
    "# Calcular media y desviaci√≥n est√°ndar de los datos de entrenamiento\n",
    "mean = np.mean(x_train, axis=(0, 1, 2))\n",
    "std = np.std(x_train, axis=(0, 1, 2))\n",
    "\n",
    "print(\"Media por canal:\", mean)\n",
    "print(\"Desviaci√≥n est√°ndar por canal:\", std)\n",
    "\n",
    "# Funci√≥n de normalizaci√≥n mejorada\n",
    "def normalize_image(image):\n",
    "    return (image - mean) / (std + 1e-7)\n",
    "\n",
    "# Normalizar datos\n",
    "x_train_normalized = normalize_image(x_train)\n",
    "x_test_normalized = normalize_image(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0837c31-48df-4425-ba6e-394256c04c76",
   "metadata": {},
   "source": [
    "## üîÑ 3. Aumento de datos avanzado (Cutout)\n",
    "Implementamos la t√©cnica de Cutout que elimina aleatoriamente secciones rectangulares de las im√°genes durante el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7b0fd1-76dd-4cb5-9e09-3488975436c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def simple_cutout(image, size=8):\n",
    "    \"\"\"Versi√≥n simplificada de cutout para menor carga computacional\"\"\"\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    \n",
    "    # Centro aleatorio\n",
    "    cx = np.random.randint(0, w)\n",
    "    cy = np.random.randint(0, h)\n",
    "    \n",
    "    # Coordenadas del recorte\n",
    "    x1 = max(0, cx - size//2)\n",
    "    x2 = min(w, cx + size//2)\n",
    "    y1 = max(0, cy - size//2)\n",
    "    y2 = min(h, cy + size//2)\n",
    "    \n",
    "    # Aplicar m√°scara\n",
    "    img_copy = image.copy()\n",
    "    img_copy[y1:y2, x1:x2, :] = 0.0\n",
    "    \n",
    "    return img_copy\n",
    "\n",
    "# Configurar el generador con menor transformaci√≥n\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=simple_cutout,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Usar batch size mayor para acelerar el entrenamiento\n",
    "batch_size = 128\n",
    "\n",
    "# Generar los generadores\n",
    "train_generator = datagen.flow(\n",
    "    x_train, y_train_cat,\n",
    "    batch_size=batch_size,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow(\n",
    "    x_train, y_train_cat,\n",
    "    batch_size=batch_size,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409868c6-eb1a-47ae-8fd4-846a85fd4d27",
   "metadata": {},
   "source": [
    "## üß± 4. Definici√≥n del bloque Inception mejorado\n",
    "Creamos una versi√≥n mejorada del bloque Inception con m√°s filtros y BatchNorm despu√©s de cada capa convolucional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac7301-221e-44a1-af25-5bd45df21629",
   "metadata": {},
   "source": [
    "### üèóÔ∏è 5. Construcci√≥n del modelo mejorado\n",
    "Construimos el modelo con mayor capacidad, usando bloques Inception mejorados y aumentando el n√∫mero de filtros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d12e6e2-7a62-41f8-b61f-fa0398f0e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 32, 32, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_21 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 32, 32, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 16, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 16, 16, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 16, 16, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_24 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_28 (Ba  (None, 8, 8, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_25 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_29 (Ba  (None, 8, 8, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 4, 4, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 256)               0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_30 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               51300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1333924 (5.09 MB)\n",
      "Trainable params: 1331108 (5.08 MB)\n",
      "Non-trainable params: 2816 (11.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def efficient_block(x, filters, kernel_size=3, regularization=1e-4):\n",
    "    \"\"\"Bloque convolucional eficiente con BatchNorm\"\"\"\n",
    "    # Primera capa convolucional\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', \n",
    "                      kernel_regularizer=regularizers.l2(regularization))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    # Segunda capa convolucional\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(regularization))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Construir modelo optimizado\n",
    "def build_efficient_model(input_shape=(32, 32, 3), num_classes=100):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Bloque 1\n",
    "    x = efficient_block(inputs, 64)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Bloque 2\n",
    "    x = efficient_block(x, 128)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Bloque 3\n",
    "    x = efficient_block(x, 256)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Clasificaci√≥n\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Crear el modelo\n",
    "model = build_efficient_model()\n",
    "\n",
    "# Mostrar resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b8ce4-9dbc-40d1-b7e4-db8b5e082606",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 7. Configuraci√≥n del optimizador y compilaci√≥n\n",
    "Configuramos un optimizador Adam con una tasa de aprendizaje optimizada y compilamos el modelo con Label Smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a47312c1-6707-4765-9e54-7ddc01dfde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos Adam para convergencia r√°pida\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compilar con label smoothing\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edc569-b725-485a-8531-1b03baa0acc7",
   "metadata": {},
   "source": [
    "## üõë 9. Configuraci√≥n de EarlyStopping mejorado\n",
    "Configuramos el callback de EarlyStopping con mayor paciencia para permitir que el modelo converja completamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba2d52a-678d-44bc-93c9-88910854b0a1",
   "metadata": {},
   "source": [
    "## üíæ 10. Configuraci√≥n de ModelCheckpoint mejorado\n",
    "Configuramos el callback de ModelCheckpoint para guardar los mejores modelos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3a8c8a-13fd-4b03-ae18-7f12e225d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Early stopping m√°s agresivo\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Checkpoint m√°s ligero (solo guarda el mejor modelo)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'checkpoints/cnn_cifar100_efficient_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reducci√≥n de tasa de aprendizaje\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb55cc0-1411-49ee-97c2-e102f9d6fdc0",
   "metadata": {},
   "source": [
    "## üß† 11. Entrenamiento del modelo mejorado\n",
    "Entrenamos el modelo utilizando los generadores de datos aumentados y los callbacks configurados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88823f2b-e07e-4c24-81fc-ea73d5fbe89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 4.4511 - accuracy: 0.0667\n",
      "Epoch 1: val_accuracy improved from -inf to 0.03890, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 278s 881ms/step - loss: 4.4511 - accuracy: 0.0667 - val_loss: 4.9652 - val_accuracy: 0.0389 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 3.9775 - accuracy: 0.1309\n",
      "Epoch 2: val_accuracy improved from 0.03890 to 0.12000, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 260s 829ms/step - loss: 3.9775 - accuracy: 0.1309 - val_loss: 4.0400 - val_accuracy: 0.1200 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 3.6700 - accuracy: 0.1953\n",
      "Epoch 3: val_accuracy improved from 0.12000 to 0.20860, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 253s 809ms/step - loss: 3.6700 - accuracy: 0.1953 - val_loss: 3.6180 - val_accuracy: 0.2086 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 3.4612 - accuracy: 0.2453\n",
      "Epoch 4: val_accuracy improved from 0.20860 to 0.22650, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 255s 815ms/step - loss: 3.4612 - accuracy: 0.2453 - val_loss: 3.5428 - val_accuracy: 0.2265 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 3.3112 - accuracy: 0.2846\n",
      "Epoch 5: val_accuracy improved from 0.22650 to 0.23430, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 255s 814ms/step - loss: 3.3112 - accuracy: 0.2846 - val_loss: 3.5802 - val_accuracy: 0.2343 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 3.2039 - accuracy: 0.3149\n",
      "Epoch 6: val_accuracy improved from 0.23430 to 0.29060, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 255s 814ms/step - loss: 3.2039 - accuracy: 0.3149 - val_loss: 3.3046 - val_accuracy: 0.2906 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 3.1140 - accuracy: 0.3398\n",
      "Epoch 7: val_accuracy did not improve from 0.29060\n",
      "313/313 [==============================] - 261s 833ms/step - loss: 3.1140 - accuracy: 0.3398 - val_loss: 3.3969 - val_accuracy: 0.2831 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 3.0467 - accuracy: 0.3622\n",
      "Epoch 8: val_accuracy improved from 0.29060 to 0.29580, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 249s 795ms/step - loss: 3.0467 - accuracy: 0.3622 - val_loss: 3.2936 - val_accuracy: 0.2958 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.9852 - accuracy: 0.3822\n",
      "Epoch 9: val_accuracy improved from 0.29580 to 0.37440, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 244s 780ms/step - loss: 2.9852 - accuracy: 0.3822 - val_loss: 2.9922 - val_accuracy: 0.3744 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.9456 - accuracy: 0.3979\n",
      "Epoch 10: val_accuracy did not improve from 0.37440\n",
      "313/313 [==============================] - 245s 784ms/step - loss: 2.9456 - accuracy: 0.3979 - val_loss: 3.1869 - val_accuracy: 0.3239 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.8945 - accuracy: 0.4150\n",
      "Epoch 11: val_accuracy improved from 0.37440 to 0.40300, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 244s 778ms/step - loss: 2.8945 - accuracy: 0.4150 - val_loss: 2.9337 - val_accuracy: 0.4030 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.8540 - accuracy: 0.4257\n",
      "Epoch 12: val_accuracy did not improve from 0.40300\n",
      "313/313 [==============================] - 248s 792ms/step - loss: 2.8540 - accuracy: 0.4257 - val_loss: 2.9584 - val_accuracy: 0.3980 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.8297 - accuracy: 0.4331\n",
      "Epoch 13: val_accuracy did not improve from 0.40300\n",
      "313/313 [==============================] - 247s 788ms/step - loss: 2.8297 - accuracy: 0.4331 - val_loss: 3.0011 - val_accuracy: 0.3884 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.7953 - accuracy: 0.4500\n",
      "Epoch 14: val_accuracy improved from 0.40300 to 0.42500, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 249s 795ms/step - loss: 2.7953 - accuracy: 0.4500 - val_loss: 2.8692 - val_accuracy: 0.4250 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.7675 - accuracy: 0.4581\n",
      "Epoch 15: val_accuracy improved from 0.42500 to 0.42940, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 245s 784ms/step - loss: 2.7675 - accuracy: 0.4581 - val_loss: 2.8821 - val_accuracy: 0.4294 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.7413 - accuracy: 0.4661\n",
      "Epoch 16: val_accuracy improved from 0.42940 to 0.44510, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 247s 789ms/step - loss: 2.7413 - accuracy: 0.4661 - val_loss: 2.8297 - val_accuracy: 0.4451 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.7134 - accuracy: 0.4789\n",
      "Epoch 17: val_accuracy did not improve from 0.44510\n",
      "313/313 [==============================] - 252s 805ms/step - loss: 2.7134 - accuracy: 0.4789 - val_loss: 2.9005 - val_accuracy: 0.4284 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.7002 - accuracy: 0.4865\n",
      "Epoch 18: val_accuracy did not improve from 0.44510\n",
      "313/313 [==============================] - 252s 805ms/step - loss: 2.7002 - accuracy: 0.4865 - val_loss: 2.8568 - val_accuracy: 0.4394 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.6872 - accuracy: 0.4932\n",
      "Epoch 19: val_accuracy improved from 0.44510 to 0.44570, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 249s 795ms/step - loss: 2.6872 - accuracy: 0.4932 - val_loss: 2.8099 - val_accuracy: 0.4457 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.6568 - accuracy: 0.5002\n",
      "Epoch 20: val_accuracy did not improve from 0.44570\n",
      "313/313 [==============================] - 251s 801ms/step - loss: 2.6568 - accuracy: 0.5002 - val_loss: 2.8550 - val_accuracy: 0.4414 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.6504 - accuracy: 0.5062\n",
      "Epoch 21: val_accuracy improved from 0.44570 to 0.46360, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 248s 791ms/step - loss: 2.6504 - accuracy: 0.5062 - val_loss: 2.7828 - val_accuracy: 0.4636 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.6369 - accuracy: 0.5099\n",
      "Epoch 22: val_accuracy improved from 0.46360 to 0.46610, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 253s 809ms/step - loss: 2.6369 - accuracy: 0.5099 - val_loss: 2.7755 - val_accuracy: 0.4661 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.6206 - accuracy: 0.5158\n",
      "Epoch 23: val_accuracy improved from 0.46610 to 0.48380, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 247s 788ms/step - loss: 2.6206 - accuracy: 0.5158 - val_loss: 2.7072 - val_accuracy: 0.4838 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.6104 - accuracy: 0.5217\n",
      "Epoch 24: val_accuracy improved from 0.48380 to 0.49070, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 248s 792ms/step - loss: 2.6104 - accuracy: 0.5217 - val_loss: 2.7099 - val_accuracy: 0.4907 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5956 - accuracy: 0.5278\n",
      "Epoch 25: val_accuracy did not improve from 0.49070\n",
      "313/313 [==============================] - 252s 804ms/step - loss: 2.5956 - accuracy: 0.5278 - val_loss: 2.9618 - val_accuracy: 0.4321 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5925 - accuracy: 0.5332\n",
      "Epoch 26: val_accuracy did not improve from 0.49070\n",
      "313/313 [==============================] - 247s 789ms/step - loss: 2.5925 - accuracy: 0.5332 - val_loss: 2.8529 - val_accuracy: 0.4581 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5750 - accuracy: 0.5378\n",
      "Epoch 27: val_accuracy did not improve from 0.49070\n",
      "313/313 [==============================] - 364s 1s/step - loss: 2.5750 - accuracy: 0.5378 - val_loss: 2.7247 - val_accuracy: 0.4845 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5626 - accuracy: 0.5422\n",
      "Epoch 28: val_accuracy improved from 0.49070 to 0.49220, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 246s 784ms/step - loss: 2.5626 - accuracy: 0.5422 - val_loss: 2.7265 - val_accuracy: 0.4922 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5602 - accuracy: 0.5426\n",
      "Epoch 29: val_accuracy improved from 0.49220 to 0.49520, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 249s 794ms/step - loss: 2.5602 - accuracy: 0.5426 - val_loss: 2.7224 - val_accuracy: 0.4952 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5527 - accuracy: 0.5441\n",
      "Epoch 30: val_accuracy improved from 0.49520 to 0.52050, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 248s 791ms/step - loss: 2.5527 - accuracy: 0.5441 - val_loss: 2.6182 - val_accuracy: 0.5205 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5466 - accuracy: 0.5484\n",
      "Epoch 31: val_accuracy did not improve from 0.52050\n",
      "313/313 [==============================] - 248s 791ms/step - loss: 2.5466 - accuracy: 0.5484 - val_loss: 2.7211 - val_accuracy: 0.4975 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5345 - accuracy: 0.5542\n",
      "Epoch 32: val_accuracy improved from 0.52050 to 0.53990, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 249s 795ms/step - loss: 2.5345 - accuracy: 0.5542 - val_loss: 2.5572 - val_accuracy: 0.5399 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5316 - accuracy: 0.5548\n",
      "Epoch 33: val_accuracy did not improve from 0.53990\n",
      "313/313 [==============================] - 245s 781ms/step - loss: 2.5316 - accuracy: 0.5548 - val_loss: 2.6351 - val_accuracy: 0.5153 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5271 - accuracy: 0.5595\n",
      "Epoch 34: val_accuracy did not improve from 0.53990\n",
      "313/313 [==============================] - 247s 788ms/step - loss: 2.5271 - accuracy: 0.5595 - val_loss: 2.7285 - val_accuracy: 0.5024 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.5170 - accuracy: 0.5641\n",
      "Epoch 35: val_accuracy did not improve from 0.53990\n",
      "313/313 [==============================] - 245s 783ms/step - loss: 2.5170 - accuracy: 0.5641 - val_loss: 2.6920 - val_accuracy: 0.5081 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.4990 - accuracy: 0.5686\n",
      "Epoch 36: val_accuracy improved from 0.53990 to 0.55500, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 247s 788ms/step - loss: 2.4990 - accuracy: 0.5686 - val_loss: 2.5435 - val_accuracy: 0.5550 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.4981 - accuracy: 0.5695\n",
      "Epoch 37: val_accuracy did not improve from 0.55500\n",
      "313/313 [==============================] - 248s 791ms/step - loss: 2.4981 - accuracy: 0.5695 - val_loss: 2.7521 - val_accuracy: 0.4991 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.4985 - accuracy: 0.5718\n",
      "Epoch 38: val_accuracy did not improve from 0.55500\n",
      "313/313 [==============================] - 248s 790ms/step - loss: 2.4985 - accuracy: 0.5718 - val_loss: 2.6567 - val_accuracy: 0.5214 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.4923 - accuracy: 0.5703\n",
      "Epoch 39: val_accuracy did not improve from 0.55500\n",
      "313/313 [==============================] - 246s 787ms/step - loss: 2.4923 - accuracy: 0.5703 - val_loss: 2.5786 - val_accuracy: 0.5407 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.4897 - accuracy: 0.5742\n",
      "Epoch 40: val_accuracy did not improve from 0.55500\n",
      "313/313 [==============================] - 248s 791ms/step - loss: 2.4897 - accuracy: 0.5742 - val_loss: 2.6097 - val_accuracy: 0.5327 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.4880 - accuracy: 0.5768\n",
      "Epoch 41: val_accuracy did not improve from 0.55500\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "313/313 [==============================] - 243s 776ms/step - loss: 2.4880 - accuracy: 0.5768 - val_loss: 2.6255 - val_accuracy: 0.5316 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.3668 - accuracy: 0.6159\n",
      "Epoch 42: val_accuracy improved from 0.55500 to 0.57800, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 247s 789ms/step - loss: 2.3668 - accuracy: 0.6159 - val_loss: 2.4535 - val_accuracy: 0.5780 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.3292 - accuracy: 0.6233\n",
      "Epoch 43: val_accuracy improved from 0.57800 to 0.59790, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 249s 794ms/step - loss: 2.3292 - accuracy: 0.6233 - val_loss: 2.3824 - val_accuracy: 0.5979 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.3095 - accuracy: 0.6269\n",
      "Epoch 44: val_accuracy did not improve from 0.59790\n",
      "313/313 [==============================] - 248s 792ms/step - loss: 2.3095 - accuracy: 0.6269 - val_loss: 2.4264 - val_accuracy: 0.5872 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.2889 - accuracy: 0.6334\n",
      "Epoch 45: val_accuracy improved from 0.59790 to 0.61080, saving model to checkpoints/cnn_cifar100_efficient_best.h5\n",
      "313/313 [==============================] - 247s 789ms/step - loss: 2.2889 - accuracy: 0.6334 - val_loss: 2.3453 - val_accuracy: 0.6108 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.2772 - accuracy: 0.6349\n",
      "Epoch 46: val_accuracy did not improve from 0.61080\n",
      "313/313 [==============================] - 247s 790ms/step - loss: 2.2772 - accuracy: 0.6349 - val_loss: 2.4100 - val_accuracy: 0.5908 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.2641 - accuracy: 0.6370\n",
      "Epoch 47: val_accuracy did not improve from 0.61080\n",
      "313/313 [==============================] - 245s 784ms/step - loss: 2.2641 - accuracy: 0.6370 - val_loss: 2.4602 - val_accuracy: 0.5717 - lr: 5.0000e-04\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.2512 - accuracy: 0.6420\n",
      "Epoch 48: val_accuracy did not improve from 0.61080\n",
      "313/313 [==============================] - 244s 781ms/step - loss: 2.2512 - accuracy: 0.6420 - val_loss: 2.4379 - val_accuracy: 0.5761 - lr: 5.0000e-04\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.2502 - accuracy: 0.6398\n",
      "Epoch 49: val_accuracy did not improve from 0.61080\n",
      "313/313 [==============================] - 246s 784ms/step - loss: 2.2502 - accuracy: 0.6398 - val_loss: 2.3903 - val_accuracy: 0.5866 - lr: 5.0000e-04\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - ETA: 0s - loss: 2.2426 - accuracy: 0.6421\n",
      "Epoch 50: val_accuracy did not improve from 0.61080\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "313/313 [==============================] - 244s 780ms/step - loss: 2.2426 - accuracy: 0.6421 - val_loss: 2.3231 - val_accuracy: 0.6092 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos con menos √©pocas pero suficientes para ver mejoras\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,  # Menos √©pocas\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9460007-a890-4f82-ad0b-fa0542c86498",
   "metadata": {},
   "source": [
    "# üìä 12. Evaluaci√≥n del modelo\n",
    "Evaluamos el modelo mejorado en el conjunto de test y comparamos los resultados con el modelo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5da709-dec3-4582-8140-43320b8a1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en el conjunto de test\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test_cat)\n",
    "print(f\"üîç Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"üìâ Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f966ad4-776e-4acd-a8f6-9c03a3226ad7",
   "metadata": {},
   "source": [
    "# üìà 13. Visualizaci√≥n de resultados\n",
    "Graficamos las m√©tricas de entrenamiento y validaci√≥n para analizar el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd513b7a-1f63-40e3-acc3-5e2fb14a7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(hist):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist.history['accuracy'], label='Entrenamiento')\n",
    "    plt.plot(hist.history['val_accuracy'], label='Validaci√≥n')\n",
    "    plt.title('Precisi√≥n')\n",
    "    plt.xlabel('√âpocas')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist.history['loss'], label='Entrenamiento')\n",
    "    plt.plot(hist.history['val_loss'], label='Validaci√≥n')\n",
    "    plt.title('P√©rdida')\n",
    "    plt.xlabel('√âpocas')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Graficar el historial de entrenamiento\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f13d28-2697-4caf-9cf6-394da6e966fa",
   "metadata": {},
   "source": [
    "# üß† 14. Conclusiones del modelo mejorado\n",
    "Analizamos las mejoras obtenidas y comparamos con el modelo base y el primer modelo mejorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624283d9-d34f-4e0b-8aab-8bcdd3c927c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
